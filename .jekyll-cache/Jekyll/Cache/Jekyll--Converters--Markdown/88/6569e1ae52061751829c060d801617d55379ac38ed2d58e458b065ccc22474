I"V=<blockquote>
  <p>I have recently came across a very interesting paper named <a href="https://arxiv.org/abs/1901.10430"><strong>Pay Less Attention with Lightweight and Dynamic Convolutions</strong></a>. The prime motivation of the paper is that <a href="https://arxiv.org/abs/1706.03762">Self-attention</a> models suffer from quadratic time complexity in terms of the the input size. This papers proposes a variant of the convolution operation named Lightweight Convolutions that scales linearly with the input size while performaing comparably with state-of-the-art self-attention models.</p>
</blockquote>

<!--more-->

<h3 id="and-what-are-these-lightweight-convolutions-anyway">And What are these Lightweight Convolutions Anyway?</h3>
<p>Given a sequence of words <code class="language-plaintext highlighter-rouge">$X \in \mathbb{R}^{n \times d}$</code>, where $n$ is the sequence length and $d$ is the word embeddings dimension, we want to tranform $X$ into an output matrix of the same shape <code class="language-plaintext highlighter-rouge">$O \in \mathbb{R}^{n \times d}$</code>:</p>

<p><strong>Self Attention</strong> : Self attention will compute a Key matrix $K$ and a query matrix $Q$ and a value matrix $V$ through linear tranformations of $X$. Then the output is computed by :</p>

<p><code class="language-plaintext highlighter-rouge">
$$
\begin{equation}
Attention(Q, K, V) = softmax (\frac{QK^{T}}{\sqrt{d_k}}) V
\end{equation}
$$
</code></p>

<p><strong>Depthwise Convolutions</strong> : While the normal convolution operation involves using filters that are as deep as the input tensor. For instance, if we’re doing a 2D Convolution on a <code class="language-plaintext highlighter-rouge">$128 \times 128 \times 32$</code> tensor, we need our filters to have dimensions <code class="language-plaintext highlighter-rouge">$k \times k \times 32$</code>, where $k$ is the filter size. Obviously, having  depth means having more parameters. Here comes the idea of Depthwise Convolutions, where instead of having deep filters, we have <strong>shallow filters</strong> operating on a slice of the depth of the input tensor. Thus, with Depthwise Convolutions, our filters each have a size of <code class="language-plaintext highlighter-rouge">$k \times k$</code></p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="/images/depthwise-conv.png" width="500" height="400" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><em>2D Depthwise Convolutions using 3 filters of size 5x5. <a href="https://towardsdatascience.com/a-basic-introduction-to-separable-convolutions-b99ec3102728">Figure source</a></em></td>
    </tr>
  </tbody>
</table>

<p>Now considering the one-dimensional case: To output a matrix <code class="language-plaintext highlighter-rouge">$O \in \mathbb{R}^{n \times d}$</code> using vanilla convolutions, we need $d$ filters each having size $d \times k$. This means that using normal convolutions, we need $d^{2} k$ parameters, while with depthwise convolutions, all we need are $d k$ parameters. See the figure below for a 1-dimensional depthwise-convolution on a $5 \times 5$ matrix with a filter of size $K=2$.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="/images/depthwise-conv-1d.gif" width="400" height="300" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><em>1D Depthwise Convolutions with a 2-sized filter</em></td>
    </tr>
  </tbody>
</table>

<p><strong>Lightweight Convolution</strong>: The proposed Lightweight Convolutions are built on top of three main components</p>

<ol>
  <li><strong>Depthwise convolutions</strong>: explained above.</li>
  <li><strong>Softmax-normalization</strong>: That is, the filter weights are normalized along the temporal timension $k$ with a <em>Softmax</em> operation. This is a novel idea altough its partly borrowed from self-attention. Softmax Normalization forces the convolution filters to compute a weighted sum of sequence elements (words, tokens, etc.) and thus learn the contribution of each element to the final output.</li>
  <li><strong>Weight Sharing</strong>: To use even fewer parameters, the filter weights are tied across the embeddings dimension $d$ by grouping together every adjacent $d/H$ weights. This shrinks the parameters required even further from $dK$ to $HK$</li>
</ol>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="/images/lightweight-module.PNG" width="250" height="200" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><em>Lightweight Convolutions Module</em></td>
    </tr>
  </tbody>
</table>

<p>As shown above, Lightweight Convolutions are preceded by a linear transformation mapping the input from dimension $d$ to $2d$, which is then followed by Gated Linear Unit (GLU). The GLU uses half of the input matrix as a gate by passing it to the sigmoid function, and then mutliplying elementwise by the second half. Then, another linear transformation is applied to obtain the output matrix $O$:</p>

<p><code class="language-plaintext highlighter-rouge">
$$
\begin{align}
\begin{split}
Q &amp;= X W_1,  \;\; W_1 \in \mathbb{R}^{d \times 2d}
\\
G &amp;= \sigma(Q_{:,:d}) \otimes Q_{:,d:2d}
\\
L &amp;= LConv(G)
\\
O &amp;= L W_o, \;\; W_o \in \mathbb{R}^{d \times d}
\\ 
\end{split}
\end{align}
$$
</code></p>

<p><strong>LConv Implementation</strong> :
Upon reading the paper, I was very interested in knowing how lightweight convolutions were implemented. So, I took a look at the module implementation on <a href="https://github.com/pytorch/fairseq/blob/72291287c8bedd868eaeb2cc9bb6a15134d1cdb5/fairseq/modules/lightweight_convolution.py">Github</a>. Interestingly, the authors implement <em>LConv</em> by transforming the convolution operation into matrix multiplication by a set of <a href="https://en.wikipedia.org/wiki/Band_matrix"><em>Band Matrices</em></a>. Band Matrices are a type of sparse matrices where non-zero entries are concentrated around the main diagonal along with any other diagonals on either side. Below is an example of band matrix. They claim that this solution is faster than the existing CUDA convolutions on shorter sequences.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2e3206cd86e5b01a5389351ad8e310665f3ff8d6" width="200" height="200" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><em>Example Band Matrix</em></td>
    </tr>
  </tbody>
</table>

<p>Now let’s understand the implementation idea using an example. Given an input $X \in \mathbb{R}^{3 \times 4}$, imagine we’re about to do a lightweight convoution with a filter of size $K=2$, and using weight sharing where $H=2$ (meaning that $W \in \mathbb{R}^{H=2 \times K=2}$) and the input dimension <code class="language-plaintext highlighter-rouge">$d=4$</code>.</p>

<p>For instance let</p>

<p><code class="language-plaintext highlighter-rouge">
$$
W = 
\begin{bmatrix}
1 &amp; 2  \\ 
1 &amp; 2  \\ 
\end{bmatrix}
$$
</code>
Convolving $W$ around $X$ gives the following results:</p>

<p><code class="language-plaintext highlighter-rouge">
$$
\begin{bmatrix}
1 &amp; 2  &amp; 3 &amp; 1 \\ 
3 &amp; 2  &amp; 1 &amp; 3\\ 
4&amp; 4  &amp; 2 &amp; 1\\ 
0 &amp; 0 &amp; 0 &amp; 0 \\
\end{bmatrix}  * 
\left[
\begin{array}{cc|cc}
1 &amp; 1  &amp; 2 &amp; 2 \\ 
1 &amp; 1  &amp; 2 &amp; 2 \\ 
\end{array}  
\right] = \begin{bmatrix}
4 &amp; 4  &amp; 8 &amp; 8 \\ 
7 &amp; 6  &amp; 6 &amp; 8\\
4 &amp; 4 &amp; 4 &amp; 2\\ 
\end{bmatrix}  
$$
</code></p>

<p>Now let’s see how we can obtain this result using the band matrices methods. The preceding operation will be divided into $H=2$ different matrix multiplications of a band matrix with the relevant part of the input matrix. Which are the outputs of convolving using $W_{1,:}$ and $W_{2,:}$, respectively:</p>

<p><code class="language-plaintext highlighter-rouge">
$$
\begin{bmatrix}
1 &amp; 1  &amp; 0 &amp; 0 \\ 
0 &amp; 1  &amp; 1 &amp; 0\\ 
0&amp; 0  &amp; 1 &amp; 1\\ 
\end{bmatrix}  
\begin{bmatrix}
1 &amp; 2 \\ 
3 &amp; 2 \\
4 &amp; 4 \\ 
0 &amp; 0 \\ 
\end{bmatrix}  = 
\begin{bmatrix}
4 &amp; 4 \\ 
7 &amp; 6 \\
4 &amp; 4 \\ 
\end{bmatrix}
$$
</code></p>

<p>and</p>

<p><code class="language-plaintext highlighter-rouge">
$$
\begin{bmatrix}
2 &amp; 2  &amp; 0 &amp; 0 \\ 
0 &amp; 2  &amp; 2 &amp; 0\\ 
0&amp; 0  &amp; 2 &amp; 2\\ 
\end{bmatrix}  
\begin{bmatrix}
3 &amp; 1 \\ 
1 &amp; 3 \\
2 &amp; 1 \\ 
0 &amp; 0 \\ 
\end{bmatrix}  = 
\begin{bmatrix}
8 &amp; 8 \\ 
6 &amp; 8 \\
4 &amp; 2 \\ 
\end{bmatrix}
$$
</code>
which are then combined to obtain the output matrix.</p>

<p>Now let’s take a look at the <em>PyTorch</em> implementation</p>

<p>The following lines expand and reshape the convolution filter weights in a way such that <em>batch matrix multiplication</em> can be done:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">weight</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="n">H</span><span class="p">,</span> <span class="n">K</span><span class="p">)</span>
<span class="n">weight</span> <span class="o">=</span> <span class="n">weight</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">K</span><span class="p">).</span><span class="n">expand</span><span class="p">(</span><span class="n">T</span><span class="o">*</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">K</span><span class="p">).</span><span class="n">contiguous</span><span class="p">()</span>
<span class="n">weight</span> <span class="o">=</span> <span class="n">weight</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">B</span><span class="o">*</span><span class="n">H</span><span class="p">,</span> <span class="n">K</span><span class="p">).</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span></code></pre></figure>

<p>Now, we need to create the band matrices using the convolution filter weights. The <code class="language-plaintext highlighter-rouge">as_strided</code> function  serves this purpose by creating a strided view into <code class="language-plaintext highlighter-rouge">weight_expanded</code>. It takes the output shape <code class="language-plaintext highlighter-rouge">(B*H, T, K)</code> and the strides or jumps we need to traverse the input array in each dimension which are :</p>
<ul>
  <li>For the <code class="language-plaintext highlighter-rouge">B*H</code> dimension we don’t need any strides so while traversing the array using that dimension, we need to jump total number of elements per one step <code class="language-plaintext highlighter-rouge">T*(T+K-1)</code>.</li>
  <li>For the temporal dimension <code class="language-plaintext highlighter-rouge">T</code>, we need a stride of <em>one</em> so we use we jump by the total number of elements <code class="language-plaintext highlighter-rouge">T+K-1</code>  our plus one element for the stride us <code class="language-plaintext highlighter-rouge">T+K</code> elements which are.</li>
  <li>for the last dimension, we have want no strides so we jump only one element at a time.</li>
</ul>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">weight_expanded</span> <span class="o">=</span> <span class="n">weight</span><span class="p">.</span><span class="n">new_zeros</span><span class="p">(</span><span class="n">B</span><span class="o">*</span><span class="n">H</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">T</span><span class="o">+</span><span class="n">K</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">weight_expanded</span><span class="p">.</span><span class="n">as_strided</span><span class="p">((</span><span class="n">B</span><span class="o">*</span><span class="n">H</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">K</span><span class="p">),</span> <span class="p">(</span><span class="n">T</span><span class="o">*</span><span class="p">(</span><span class="n">T</span><span class="o">+</span><span class="n">K</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">T</span><span class="o">+</span><span class="n">K</span><span class="p">,</span> <span class="mi">1</span><span class="p">)).</span><span class="n">copy_</span><span class="p">(</span><span class="n">weight</span><span class="p">)</span><span class="c1"># creating the band matrices using the
# as_strided function
</span><span class="n">weight_expanded</span> <span class="o">=</span> <span class="n">weight_expanded</span><span class="p">.</span><span class="n">narrow</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">P</span><span class="p">,</span> <span class="n">T</span><span class="p">)</span></code></pre></figure>

<h3 id="ok-how-about-dynamic-convolutions">Ok, How about Dynamic Convolutions?</h3>
<p>One limitation of Lightweight Convolutions as opposed to self-attention is that the weights assigned to sequence elements do not change according to context. That is, the convolution weights used at the timestep $i$ are the same when sliding the filter one more step at $i+1$.</p>

<p>Dynamic Convolutions remedy that by using a <em>time-dependent</em> kernel that changes dynamically based on the current timestep (Not the whole context as with self-attention):</p>

<p><code class="language-plaintext highlighter-rouge">
$$
\begin{align}
DynamicConv(X, i, c) = LightConv(X, f(Xi)_{h,:},i,c)
\end{align}
$$
</code></p>

<p>where $f(X_i)$ is a matrix computed by means of a linear transformation with learnt weight matrix $W^Q \in \mathbb{R}^{H \times k \times d}$. Thus, the filter weight for positions for position $j$ and head $h$ are computed by
\(\sum_{c=1}^{d}W_{h,j,c}^{Q}. X_{i,c}\).</p>

<p>Note that to compute the attention weights, the above equation will be computed at $O(n)$ times in contrast to computing self-attention which computed them in $O(n^2)$ (The cost of multiplying $KQ^{T}$. Dynamic Convolutions are very memory hungry (just look at $W^Q$!). So, it was essential to shrink the number parameters through both Depthwise operation and weight sharing, in order to be able to implement Dynamic Convolutions on current hardware.</p>

<h3 id="conclusion">Conclusion</h3>
<p>In this article, we saw how the simple but interesting idea of Lightweight Convolutions originated. We also discussed how it is partly based on Depthwise Convolutions, Softmax normalization and weight sharing. We delved into the implementation idea and source code used by the authors where convolution was implemented as a batch matrix multiplication by a set of band matrices.
Finally, we explored the interesting concept of time-depended kernels where convolution weights are not fixed but rather depend on the current timestep.</p>
:ET
I"Åu<p>In this post, I will walk you through using Tensorflow to classify news articles.
Before you begin, you should have tensorflow, numpy and scikit-learn installed.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td><td class="rouge-code"><pre><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_20newsgroups</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">TfidfVectorizer</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<h3 id="importing-twenty-newsgroup-dataset">Importing twenty-newsgroup dataset</h3>
<p>the twenty-newsgorup datasets is a collection of news articles.<br />
Each article is labeled according to its type for example : medical news, automobile news,‚Ä¶etc.<br />
In this tutorial we will deal with 4 classes of articles.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="rouge-code"><pre><span class="n">categories</span> <span class="o">=</span> <span class="p">[</span><span class="s">'rec.autos'</span><span class="p">,</span> <span class="s">'comp.graphics'</span><span class="p">,</span> <span class="s">'sci.med'</span><span class="p">,</span> <span class="s">'sci.electronics'</span><span class="p">]</span>
<span class="n">no_classes</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">categories</span><span class="p">)</span>
<span class="n">twenty_train</span> <span class="o">=</span> <span class="n">fetch_20newsgroups</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="s">'train'</span><span class="p">,</span>
                                  <span class="n">categories</span><span class="o">=</span><span class="n">categories</span><span class="p">,</span>
                                  <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">twenty_test</span> <span class="o">=</span> <span class="n">fetch_20newsgroups</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="s">'test'</span><span class="p">,</span><span class="n">categories</span><span class="o">=</span><span class="n">categories</span><span class="p">,</span><span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<h3 id="tf-idf-statistic">TF-IDF statistic</h3>
<p>We will use the bag-of-words model to represent each article.</p>

<p>There are a couple of variations of that model: term frequency , which represents each document as a vector of word counts, also there is term frequency-inverse document frequency which is the same as tf except that each word is weighted by its significance to that article.</p>

<p>Luckily for us, scikit-learn has a class just for that.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td><td class="rouge-code"><pre><span class="n">tfidf</span> <span class="o">=</span> <span class="n">TfidfVectorizer</span><span class="p">()</span>
<span class="n">train_x</span> <span class="o">=</span> <span class="n">tfidf</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">twenty_train</span><span class="p">.</span><span class="n">data</span><span class="p">)</span><span class="c1"># fit on training data
</span><span class="n">train_y</span> <span class="o">=</span> <span class="n">twenty_train</span><span class="p">.</span><span class="n">target</span> <span class="c1"># train target values
</span><span class="n">test_x</span> <span class="o">=</span> <span class="n">tfidf</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">twenty_test</span><span class="p">.</span><span class="n">data</span><span class="p">)</span> <span class="c1"># transform test data to tfidf representation
</span><span class="n">test_y</span> <span class="o">=</span> <span class="n">twenty_test</span><span class="p">.</span><span class="n">target</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<h3 id="preparing-data-for-training">Preparing data for training</h3>
<p>When doing classification using neural networks, we must have an output layer with k neurons where k is the total number of classes. If an instance belongs to a certain class, the output value for the corresponding neuron for that class should be 1 and all other values should be zero. Thus to be able to train our neural network, we have to transform the class labels of the articles to a vector having 1 at the correspoing label and 0 at all other positions. This type of vectors is known as one-hot vector.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre></td><td class="rouge-code"><pre><span class="c1"># transforming target classes into one-hot vectors
</span><span class="k">def</span> <span class="nf">vector_to_one_hot</span><span class="p">(</span><span class="n">vector</span><span class="p">,</span><span class="n">no_classes</span><span class="p">):</span>
    <span class="n">vector</span><span class="o">=</span><span class="n">vector</span><span class="p">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">vector</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">no_classes</span><span class="p">))</span>
    <span class="n">m</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">vector</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">vector</span><span class="p">]</span><span class="o">=</span><span class="mi">1</span>
    <span class="k">return</span> <span class="n">m</span>

<span class="n">train_y</span> <span class="o">=</span><span class="n">vector_to_one_hot</span><span class="p">(</span><span class="n">train_y</span><span class="p">,</span><span class="n">no_classes</span><span class="p">)</span>
<span class="n">test_y</span> <span class="o">=</span> <span class="n">vector_to_one_hot</span><span class="p">(</span><span class="n">test_y</span><span class="p">,</span> <span class="n">no_classes</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>We‚Äôll use a very simple architecture like the one shown in the picture. 
However, we‚Äôll an input layer of size equal to the vocabulary size, A hidden layer of our choice and
an output layer of 4 output neurons corresponding to the 4 classes we have.</p>

<p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/e/e4/Artificial_neural_network.svg/400px-Artificial_neural_network.svg.png" alt="neural network" /></p>

<p><a href="https://en.wikibooks.org/wiki/Artificial_Neural_Networks/Neural_Network_Basics"><em>photo credit</em></a></p>

<h3 id="setting-model-parameters">Setting model parameters</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
</pre></td><td class="rouge-code"><pre><span class="c1"># Parameters
</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">num_steps</span> <span class="o">=</span> <span class="mi">500</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">16</span>
<span class="n">display_step</span> <span class="o">=</span> <span class="mi">100</span>

<span class="n">beta</span> <span class="o">=</span> <span class="mi">1</span> <span class="c1"># regularization parameter
# Network Parameters
</span><span class="n">n_hidden_1</span> <span class="o">=</span> <span class="mi">8</span> <span class="c1"># size of 1st hidden layer
</span>
<span class="n">num_input</span> <span class="o">=</span> <span class="n">train_x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="c1">#input vector size
</span><span class="n">num_classes</span> <span class="o">=</span> <span class="n">no_classes</span> 
</pre></td></tr></tbody></table></code></pre></div></div>

<h3 id="defining-input-and-output">Defining input and output</h3>

<p>With tensorflow, inputs and outputs to network are defined as placeholders.<br />
The training data is the fed to these placeholder at training time.<br />
Note we define the first dimension to be None meaning we don‚Äôt have a fixed input size (batch size) and tensorflow will deduce that at training time.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre><span class="n">X</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">placeholder</span><span class="p">(</span><span class="s">"float"</span><span class="p">,</span> <span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="n">num_input</span><span class="p">])</span> <span class="c1"># place holder for nn input
</span><span class="n">Y</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">placeholder</span><span class="p">(</span><span class="s">"float"</span><span class="p">,</span> <span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">])</span> <span class="c1"># place holder for nn output
</span></pre></td></tr></tbody></table></code></pre></div></div>

<h3 id="defining-weights">Defining weights</h3>
<p>Since our network has only one layer (you can add layer if you want), we will have two sets of weights : From input layer to the hidden layer and from the hidden layer to the output layer.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre></td><td class="rouge-code"><pre><span class="n">weights</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">'h1'</span> <span class="p">:</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">random_normal</span><span class="p">([</span><span class="n">num_input</span><span class="p">,</span> <span class="n">n_hidden_1</span><span class="p">])),</span>
    <span class="s">'out'</span><span class="p">:</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">random_normal</span><span class="p">([</span><span class="n">n_hidden_1</span><span class="p">,</span><span class="n">num_classes</span><span class="p">]))</span>
<span class="p">}</span>

<span class="n">biases</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">'b1'</span><span class="p">:</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">random_normal</span><span class="p">([</span><span class="n">n_hidden_1</span><span class="p">])),</span>
    <span class="s">'out'</span><span class="p">:</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">random_normal</span><span class="p">([</span><span class="n">num_classes</span><span class="p">]))</span>
<span class="p">}</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<h3 id="tie-everything-together">Tie everything together</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">neural_net</span> <span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="n">layer_1</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">weights</span><span class="p">[</span><span class="s">'h1'</span><span class="p">]),</span><span class="n">biases</span><span class="p">[</span><span class="s">'b1'</span><span class="p">])</span>
    <span class="n">layer_1</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">layer_1</span><span class="p">)</span>
    <span class="n">out_layer</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">layer_1</span><span class="p">,</span><span class="n">weights</span><span class="p">[</span><span class="s">'out'</span><span class="p">]),</span> <span class="n">biases</span><span class="p">[</span><span class="s">'out'</span><span class="p">])</span>
    <span class="n">out_layer</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">out_layer</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out_layer</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>We define our loss function to be cross entropy with softmax probabilities. Tensorflow has a function to compute that. 
The idea of using cross entropy is to maximize the probability of the correct class. So by minimizing the loss (negative log probability), we are maximizing the probabilty of the correct label.<br />
Optimization is done by means of the ADAM optimization method which is somehow more efficient than SGD.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
</pre></td><td class="rouge-code"><pre><span class="n">logits</span> <span class="o">=</span> <span class="n">neural_net</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">reduce_mean</span><span class="p">(</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">softmax_cross_entropy_with_logits</span><span class="p">(</span><span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">,</span><span class="n">labels</span><span class="o">=</span><span class="n">Y</span><span class="p">))</span>

<span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">train</span><span class="p">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">)</span>

<span class="n">train_step</span> <span class="o">=</span> <span class="n">optimizer</span><span class="p">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>

<span class="c1">#evaluate model
</span><span class="n">correct_pred</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">equal</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">tf</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">cast</span><span class="p">(</span><span class="n">correct_pred</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">))</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">init</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">global_variables_initializer</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>To get less training time, we divide training data into batches and perform all weights updates for all instances in the batch at one.</p>

<p>This method returns the next batch of given training data with the specified batch size</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
</pre></td><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">get_train_batch</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span><span class="n">train_x</span><span class="p">,</span><span class="n">train_y</span><span class="p">):</span>
    <span class="k">global</span> <span class="n">train_index</span>
    
    <span class="k">if</span> <span class="n">train_index</span> <span class="o">+</span> <span class="n">batch_size</span> <span class="o">&gt;=</span> <span class="n">train_x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
        <span class="n">train_index</span> <span class="o">+=</span> <span class="n">batch_size</span>
        <span class="k">return</span> <span class="n">train_x</span><span class="p">[</span><span class="n">train_index</span><span class="p">:,:],</span><span class="n">train_y</span><span class="p">[</span><span class="n">train_index</span><span class="p">:,:]</span><span class="c1"># false to indicate no more training batches
</span>    <span class="k">else</span> <span class="p">:</span>
        <span class="n">r</span><span class="o">=</span> <span class="n">train_x</span><span class="p">[</span><span class="n">train_index</span><span class="p">:</span><span class="n">train_index</span><span class="o">+</span><span class="n">batch_size</span><span class="p">,:],</span><span class="n">train_y</span><span class="p">[</span><span class="n">train_index</span><span class="p">:</span><span class="n">train_index</span><span class="o">+</span><span class="n">batch_size</span><span class="p">,:]</span>
        <span class="n">train_index</span> <span class="o">+=</span> <span class="n">batch_size</span>
        <span class="k">return</span> <span class="n">r</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
</pre></td><td class="rouge-code"><pre><span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="n">sess</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">init</span><span class="p">)</span>
    <span class="n">train_index</span><span class="o">=</span><span class="mi">0</span>
    <span class="n">moreTrain</span> <span class="o">=</span> <span class="bp">True</span>
    <span class="n">step</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
        <span class="n">step</span><span class="o">+=</span><span class="mi">1</span>
        <span class="k">if</span> <span class="n">train_index</span> <span class="o">&gt;=</span> <span class="n">train_x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="p">:</span> <span class="k">break</span>
        <span class="n">batch_x</span> <span class="p">,</span> <span class="n">batch_y</span> <span class="o">=</span> <span class="n">get_train_batch</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span><span class="n">train_x</span><span class="p">.</span><span class="n">todense</span><span class="p">(),</span><span class="n">train_y</span><span class="p">)</span>
        <span class="n">sess</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">train_step</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span><span class="n">batch_x</span><span class="p">,</span><span class="n">Y</span><span class="p">:</span><span class="n">batch_y</span><span class="p">})</span>
        
        <span class="k">if</span> <span class="n">step</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span> <span class="p">:</span>
            
            <span class="n">cur_loss</span><span class="p">,</span><span class="n">cur_accuracy</span> <span class="o">=</span> <span class="n">sess</span><span class="p">.</span><span class="n">run</span><span class="p">([</span><span class="n">loss</span><span class="p">,</span><span class="n">accuracy</span><span class="p">],</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span><span class="n">batch_x</span><span class="p">,</span><span class="n">Y</span><span class="p">:</span><span class="n">batch_y</span><span class="p">})</span> 
            <span class="k">print</span> <span class="p">(</span><span class="s">'loss = %.2f , accuracy = %.2f , at step %d'</span> <span class="o">%</span><span class="p">(</span><span class="n">cur_loss</span><span class="p">,</span> <span class="n">cur_accuracy</span><span class="p">,</span><span class="n">step</span><span class="p">))</span>
    

    <span class="k">print</span> <span class="p">(</span><span class="s">"done optimization"</span><span class="p">)</span>
    <span class="n">y_p</span> <span class="o">=</span> <span class="n">sess</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span><span class="n">test_x</span><span class="p">.</span><span class="n">todense</span><span class="p">(),</span>
                                      <span class="n">Y</span><span class="p">:</span><span class="n">test_y</span><span class="p">})</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"Testing Accuracy:"</span><span class="p">,</span> \
        <span class="n">sess</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">accuracy</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">test_x</span><span class="p">.</span><span class="n">todense</span><span class="p">(),</span>
                                      <span class="n">Y</span><span class="p">:</span> <span class="n">test_y</span><span class="p">}))</span>
    <span class="k">print</span> <span class="p">(</span><span class="s">"f1 score : "</span><span class="p">,</span> 
          <span class="n">metrics</span><span class="p">.</span><span class="n">f1_score</span><span class="p">(</span><span class="n">twenty_test</span><span class="p">.</span><span class="n">target</span><span class="p">,</span> <span class="n">y_p</span><span class="p">,</span><span class="n">average</span><span class="o">=</span><span class="s">'macro'</span><span class="p">))</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
</pre></td><td class="rouge-code"><pre>loss = 1.30 , accuracy = 0.38 , at step 10
loss = 1.18 , accuracy = 0.88 , at step 20
loss = 1.02 , accuracy = 0.75 , at step 30
loss = 0.97 , accuracy = 0.88 , at step 40
loss = 0.90 , accuracy = 0.88 , at step 50
loss = 0.84 , accuracy = 0.94 , at step 60
loss = 0.86 , accuracy = 0.88 , at step 70
loss = 0.77 , accuracy = 1.00 , at step 80
loss = 0.83 , accuracy = 0.94 , at step 90
loss = 0.80 , accuracy = 1.00 , at step 100
loss = 0.75 , accuracy = 1.00 , at step 110
loss = 0.76 , accuracy = 1.00 , at step 120
loss = 0.78 , accuracy = 0.94 , at step 130
loss = 0.84 , accuracy = 0.88 , at step 140
done optimization
Testing Accuracy: 0.914867
f1 score :  0.914819748906
</pre></td></tr></tbody></table></code></pre></div></div>

<p>We can see we have a very good accuracy and f1-score on test data.</p>

<h3 id="comparing-to-svm">Comparing to SVM</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre></td><td class="rouge-code"><pre><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s">'linear'</span><span class="p">)</span>
<span class="n">clf</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_x</span><span class="p">,</span><span class="n">twenty_train</span><span class="p">.</span><span class="n">target</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre>SVM Testing accuracy :  0.91041931385
SVM F1-score :  0.910699838044
</pre></td></tr></tbody></table></code></pre></div></div>

<p>we can see the NN approach slightly outperforms the SVM.
I strongly suggest You try on your own to play a little with the neural network layer structure and hyperparatmers and try to further improve its performance. You can also add regularization and see how it goes.</p>

<h3 id="conclusion">Conclusion</h3>
<p>In this tutorial we applied a simple neural network model on text classification. We represented our articles using TF-IDF vector space represenation. We then used cross entropy as our loss function. We trained the model and got very good accuracy and f1-score. We also tried and SVM model on the data and compared perfomance between the two models.</p>
:ET